{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will cover numerpus strategies for transforming raw numerical data into features purpose-built for machine leaning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescaling a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to rescale the values of a numerical feature to be between two values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's 'MinMaxScaler' to rescale a feature array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28571429],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load library\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Create a example feature\n",
    "feature = np.array([[-500.5],[-100.1],[0],[100.1],[900.9]])\n",
    "\n",
    "#Create scaler\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "#scale feature\n",
    "scale_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "#show scale feature \n",
    "scale_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to transform a feature to have a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's 'StandardScaler' performs both transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.81408043, -1.40182605, -0.53728211],\n",
       "       [-0.79153472,  1.29399328, -0.3548876 ],\n",
       "       [ 1.43823581,  0.75482941, -0.4851694 ],\n",
       "       [ 0.98596891,  0.21566555, -0.61545119],\n",
       "       [-0.81858957, -0.86266219,  1.99279031]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load library \n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Create a example feature \n",
    "feature = np.array([[1,2,3],[6,7,10],[500.5,6,5],[400.2,5,0],[0,3,100.1]])\n",
    "\n",
    "#Create scaler \n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#Transform the feature\n",
    "standardized_feature = standard_scaler.fit_transform(feature)\n",
    "\n",
    "#show feature\n",
    "standardized_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.0\n",
      "Standard deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "#print mean and standard deviation\n",
    "print(\"mean:\",round(standardized_feature.mean()))\n",
    "print(\"Standard deviation:\",standardized_feature.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our data has significant outliers, it can negatively impact our standardization by affecting the feature's mean and variance. In this scenario, it is often helpful to instead rescale the feature using the median and quartile range. In scikit-learn, we do this using the 'RobustScaler' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.25250501e-02, -1.00000000e+00, -2.85714286e-01],\n",
       "       [ 0.00000000e+00,  6.66666667e-01,  7.14285714e-01],\n",
       "       [ 1.23872745e+00,  3.33333333e-01,  0.00000000e+00],\n",
       "       [ 9.87474950e-01,  0.00000000e+00, -7.14285714e-01],\n",
       "       [-1.50300601e-02, -6.66666667e-01,  1.35857143e+01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create scaler\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "#Transform feature \n",
    "robust_scaler.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to rescale the feature values of observations to have unit norm(a total length of 1)(正则化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load libraries \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Create example matrix\n",
    "feature = np.array([[0.5,0.5],[1.1,3.4],[1.5,20.2],[1.63,34.4],[10.9,3.3]])\n",
    "\n",
    "#Create normalizer\n",
    "nomalizer = Normalizer(norm=\"l2\")\n",
    "\n",
    "#Transform feature\n",
    "nomalizer.transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform feature \n",
    "features_l2_norm = Normalizer(norm=\"l2\").transform(feature)\n",
    "\n",
    "features_l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.06912442, 0.93087558],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform feature \n",
    "features_l1_norm = Normalizer(norm=\"l1\").transform(feature)\n",
    "\n",
    "features_l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the first observation's values: 1.0\n"
     ]
    }
   ],
   "source": [
    "#print sum\n",
    "print(\"Sum of the first observation\\'s values:\",\n",
    "     features_l1_norm[0,1]+features_l1_norm[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Polynomial and Interaction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though some choose to create polynomial and interaction features manually,scikit-learn offers a built-in method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  4.,  6.,  9.],\n",
       "       [ 4.,  5., 16., 20., 25.],\n",
       "       [ 3.,  4.,  9., 12., 16.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load libraries \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Create feature matrix\n",
    "feature = np.array([[2,3],[4,5],[3,4]])\n",
    "\n",
    "#Create PolynomialFeatures object\n",
    "polynomial_interaction = PolynomialFeatures(degree=2,include_bias=False)\n",
    "\n",
    "#Create polynomial feature\n",
    "polynomial_interaction.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"degree=2\"-->raised to the second power,\"degree=3\"-->raised to the second and third power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.,   3.,   4.,   6.,   9.,   8.,  12.,  18.,  27.],\n",
       "       [  4.,   5.,  16.,  20.,  25.,  64.,  80., 100., 125.],\n",
       "       [  3.,   4.,   9.,  12.,  16.,  27.,  36.,  48.,  64.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_interaction3 = PolynomialFeatures(degree=3,include_bias=False)\n",
    "polynomial_interaction3.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  6.],\n",
       "       [ 4.,  5., 20.],\n",
       "       [ 3.,  4., 12.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = PolynomialFeatures(degree=2,interaction_only=True,include_bias=False)\n",
    "interaction.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to make a custom transformation to one or more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Create feature matrix\n",
    "feature = np.array([[2,3],[2,3],[2,3]])\n",
    "\n",
    "#Define a simple function\n",
    "def add_ten(x):\n",
    "    return x+10\n",
    "\n",
    "#Create transformer \n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "\n",
    "#Transform feature matrix\n",
    "ten_transformer.transform(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the same transformation in pandas using \"apply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2\n",
       "0        12        13\n",
       "1        12        13\n",
       "2        12        13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load libraries \n",
    "import pandas as pd\n",
    "\n",
    "#Create data frame\n",
    "df = pd.DataFrame(feature,columns=[\"feature1\",\"feature2\"])\n",
    "df.apply(add_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delecting Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common method is to assume the data is normally distributed and based on that assumption \"draw\" an ellipse around the data, classifying any observation inside the ellipse as an inlier(labeled as 1) and any observation outside the ellipse as an outlier(labeled as -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.83198811,  3.52863145],\n",
       "       [-2.76017908,  5.55121358],\n",
       "       [-1.61734616,  4.98930508],\n",
       "       [-0.52579046,  3.3065986 ],\n",
       "       [ 0.08525186,  3.64528297],\n",
       "       [-0.79415228,  2.10495117],\n",
       "       [-1.34052081,  4.15711949],\n",
       "       [-1.98197711,  4.02243551],\n",
       "       [-2.18773166,  3.33352125],\n",
       "       [-0.19745197,  2.34634916]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load libraries \n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "#Create simulated data\n",
    "feature,_=make_blobs(n_samples=10,\n",
    "                     n_features=2,\n",
    "                     centers=1,\n",
    "                     random_state=1)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1, -1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace the first observation's values with extreme values\n",
    "feature[4,0]=1000\n",
    "feature[4,1]=1000\n",
    "\n",
    "#Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination = .1)\n",
    "\n",
    "#fit detector\n",
    "outlier_detector.fit(feature)\n",
    "\n",
    "#predict outlier\n",
    "outlier_detector.predict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
